{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_cleaning_for_SBNC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liesenf/SpokenBNC-tools/blob/master/Data_cleaning_ngrams_and_visualization_for_SBNC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6UhKD4r2ZjI",
        "colab_type": "text"
      },
      "source": [
        "# **NLTK corpus statistics:**\n",
        "\n",
        "convert csv to txt and use NLTK to analyze\n",
        "\n",
        "\n",
        "\n",
        "TODO LIST\n",
        "\n",
        "1. ***(**DONE**)*** run total word frequency for all .txt files and output the frequency items and numbers in two column .csv   ***(**DONE**)***\n",
        "\n",
        "2. ***(**DONE**)***edit tokenizer so that in-utterance 2gram and 3gram dont contain \"do\" n't\" or \"gon\" \"na\" (but \"don't\" and \"gonna\" instead)***(**DONE**)***\n",
        "\n",
        "http://www.nltk.org/api/nltk.tokenize.html\n",
        "\n",
        "      2.1 if possible use MWETokenizer \"a\" \"little\" \"bit\" -> \"a_little_bit\" ?? list MWEs manually, or find MWE list if there is? what about items like Harry_Potter, community_service, ...\n",
        "\n",
        "\n",
        "3. ***(**DONE**)***run total (in-utterance) n-gram frequency for all .txt files and output the frequency items and numbers in two column .csv (same as 1. just for the ngrams)***(**DONE**)***\n",
        "\n",
        "4. how about global frequencies, global in-utterance ngrams? for reference?\n",
        "   \n",
        "\n",
        "what we need are csv files with all this info:\n",
        "\n",
        "ABSOLUTE TERMS:\n",
        "\n",
        "* absolute_term_frequency_in_gender_f\n",
        "* absolute_term_frequency_in_gender_m\n",
        "* absolute_term_frequency_in_agerange_young\n",
        "* absolute_term_frequency_in_agerange_old\n",
        "* absolute_term_frequency_in_f_young\n",
        "* absolute_term_frequency_in_m_young\n",
        "* absolute_term_frequency_in_f_old\n",
        "* absolute_term_frequency_in_m_old\n",
        "\n",
        "ABSOLUTE 2gram:\n",
        "\n",
        "\n",
        "   * absolute_2gram_frequency_in_gender_f\n",
        "   * absolute_2gram_frequency_in_gender_m\n",
        "   * absolute_2gram_frequency_in_agerange_young\n",
        "   * absolute_2gram_frequency_in_agerange_old\n",
        "   * absolute_2gram_frequency_in_f_young\n",
        "   * absolute_2gram_frequency_in_m_young\n",
        "   * absolute_2gram_frequency_in_f_old\n",
        "   * absolute_2gram_frequency_in_m_old\n",
        "\n",
        "ABSOLUTE 3gram:\n",
        "\n",
        "   * absolute_3gram_frequency_in_gender_f\n",
        "   * absolute_3gram_frequency_in_gender_m\n",
        "   * absolute_3gram_frequency_in_agerange_young\n",
        "   * absolute_3gram_frequency_in_agerange_old\n",
        "   * absolute_3gram_frequency_in_f_young\n",
        "   * absolute_3gram_frequency_in_m_young\n",
        "   * absolute_3gram_frequency_in_f_old\n",
        "   * absolute_3gram_frequency_in_m_old\n",
        "***(**DONE**)***\n",
        "\n",
        "NEXT STEP:\n",
        "\n",
        "5. after getting all the frequency tables.. if u still have time and energy, u can plot pairs as a bar chart using the \"# data visualization\" code: m vs f, old vs young etc.. THANKS!!! :)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2X5VsJZ3CX8",
        "colab_type": "text"
      },
      "source": [
        "## **1. load the csv and save \"one_line_list\" txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKrfDoss2HKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "This script reads csv files and writes all rows of column x as a txt file.\n",
        "\n",
        "Created June 2020 at HK PolyU\n",
        "@authors Andreas Liesenfeld and Gabor Parti\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "##path to file\n",
        "path = '/home/andreas/Desktop/SpokenBNC/spoken/files_by_category/'\n",
        "path_to_output = '/home/andreas/Desktop/SpokenBNC/spoken/files_by_category/'\n",
        "\n",
        "df = pd.read_csv('young_m.csv', index_col=None, header=0)\n",
        "df['utterance'].dropna(inplace=True)\n",
        "temp_list = df['utterance'].tolist()\n",
        "\n",
        "one_line_list = []\n",
        "\n",
        "for element in temp_list:\n",
        "    one_line_list.append(element.rstrip())\n",
        "\n",
        "with open(\"one_line_list.txt\", \"w\") as output:\n",
        "    output.write(str(one_line_list))\n",
        "#one_line_list will be a List of strings, containing all the utterances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTkIDOVf3HR9",
        "colab_type": "text"
      },
      "source": [
        "## **2. tokenize with nltk**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNSWLz8Z3JDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "006169e4-33c3-4781-cc8f-daf33b0429ff"
      },
      "source": [
        "#In this section we tokenize our one_line_list, which is a list of strings (utterances)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# # NLTK WORD TOKENIZER (WORKS, BUT TOO MUCH SPLITTING) can be deleted if tweet tokenizer or spacy is okay\n",
        "#from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "#tokenized_list = [word_tokenize(i) for i in one_line_list]\n",
        "\n",
        "# # TWEET TOKENIZER (WORKS NICELY, CONTRACTIONS ARE KEPT TOGETHER)\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "tokenized_list = [tknzr.tokenize(i) for i in one_line_list]\n",
        "\n",
        "# # NLTK TOKTOK TOKENIZER (HAVEN'T TRIED)\n",
        "#from nltk.tokenize.toktok import ToktokTokenizer\n",
        "#toktok = ToktokTokenizer()\n",
        "#tokenized_list = [toktok.tokenize(i) for i in one_line_list]\n",
        "\n",
        "# # SPACY TOKENIZER (NOT YET WORKING)\n",
        "# import spacy\n",
        "# from spacy.tokens import Doc\n",
        "# class WhitespaceTokenizer(object):\n",
        "#     def __init__(self, vocab):\n",
        "#         self.vocab = vocab\n",
        "#     def __call__(self, text):\n",
        "#         words = text.split(' ')\n",
        "#         # All tokens 'own' a subsequent space character in this tokenizer\n",
        "#         spaces = [True] * len(words)\n",
        "#         return Doc(self.vocab, words=words, spaces=spaces)\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
        "# doc = [nlp(i) for i in one_line_list]\n",
        "# print([t.text for t in doc])\n",
        "\n",
        "with open(\"tokenized_list.txt\", \"w\") as output:\n",
        "  output.write(str(tokenized_list))\n",
        "#the tokenized_list will be a list of lists, containing the tokenized utterances "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-3myIBGY3Sk",
        "colab_type": "text"
      },
      "source": [
        "## **2b. fine-tune the tokenization by defining MWEs (multi-word expressions) with nltk (optional)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wadbG_9TbdGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this section we can manually add multi-word expressions to be considered as one token.\n",
        "# Note, that the MWETokenizer of nltk needs an already tokenized input.\n",
        "\n",
        "# # NLTK MWE RE-TOKENIZER (WORKS)\n",
        "from nltk.tokenize import MWETokenizer\n",
        "tokenizer = MWETokenizer([('a', 'little', 'bit'), ('a', 'lot', 'of'), ('in', 'spite', 'of')])\n",
        "tokenizer.add_mwe(('Harry', 'Potter'))\n",
        "\n",
        "retokenized_list = [tokenizer.tokenize(i) for i in tokenized_list]\n",
        "#for i in retokenized_list:\n",
        "#  print(i)\n",
        "\n",
        "with open(\"retokenized_list.txt\", \"w\") as output:\n",
        "  output.write(str(retokenized_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpQ_81QixD6L",
        "colab_type": "text"
      },
      "source": [
        "## **3. word frequency in NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHNEP7jcukgC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "deb29745-4cbb-41b7-bc7e-0dea2943824b"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "#here we make a flat list from a list of lists, we only need this for the frequencies\n",
        "tokenized_flat_list = []\n",
        "for sublist in tokenized_list:\n",
        "    for item in sublist:\n",
        "        tokenized_flat_list.append(item)\n",
        "\n",
        "with open(\"tokenized_flat_list.txt\", \"w\") as output:\n",
        "    output.write(str(tokenized_flat_list))\n",
        "#this tokenized_flat_list.txt a list of tokens, it is needed to count the frequency distribution of tokens\n",
        "\n",
        "fd = nltk.FreqDist(tokenized_flat_list)\n",
        "print (fd.most_common(100)) \n",
        "#this gives us the 100 most frequent tokens for the group\n",
        "\n",
        "# NEW++NEW++NEW write complete FreqDist() output to csv file \n",
        "df_fdist = pd.DataFrame.from_dict(fd, orient='index')\n",
        "df_fdist.columns = ['Frequency']\n",
        "df_fdist.index.name = 'Term'\n",
        "df_fdist.to_csv('absolute_term_frequency_agerange_old.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 51925), ('the', 43230), ('you', 38503), ('yeah', 36470), ('a', 35242), ('and', 34661), ('it', 34498), ('?', 34110), ('pauseshort', 30339), ('to', 28788), ('like', 28501), ('that', 24895), ('of', 21101), (\"it's\", 19788), ('unclearutt', 17709), ('in', 16972), ('was', 16579), ('vocallaugh', 16245), ('so', 14954), ('is', 14818), ('but', 14210), ('just', 14190), ('no', 13788), ('anoninfo', 12278), ('oh', 11820), ('do', 11491), ('have', 11314), ('they', 10883), ('we', 10739), ('what', 10638), ('not', 10204), ('on', 10034), ('know', 9968), ('truncstart', 9537), ('he', 9509), (\"don't\", 9384), ('be', 9097), ('er', 8843), ('for', 8792), (\"that's\", 8657), ('well', 8364), ('one', 8216), ('think', 8155), ('this', 7854), ('truncend', 7800), ('erm', 7714), (\"I'm\", 7460), ('mm', 7306), ('really', 7167), ('if', 7141), ('then', 7097), ('got', 6961), ('there', 6905), ('or', 6834), ('get', 6800), ('my', 6540), ('can', 6188), ('with', 6065), ('are', 6062), ('at', 5752), ('go', 5695), ('all', 5538), ('cos', 5473), ('as', 5211), ('me', 5180), ('about', 5169), ('up', 4543), ('she', 4526), ('when', 4508), ('good', 4492), ('your', 4365), ('out', 4220), ('did', 4104), ('them', 4035), ('right', 3921), (\"I've\", 3853), ('okay', 3831), ('people', 3829), (\"you're\", 3822), ('how', 3778), (\"there's\", 3765), ('were', 3761), (\"he's\", 3736), ('had', 3705), ('would', 3562), ('mean', 3508), ('bit', 3498), ('gonna', 3406), (\"they're\", 3306), ('some', 3301), ('two', 3217), ('something', 3171), ('now', 3040), ('going', 3022), ('an', 2996), ('from', 2967), ('because', 2894), (\"didn't\", 2858), ('yes', 2758), ('see', 2739)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wYosQEpMowt",
        "colab_type": "text"
      },
      "source": [
        "## **4. ngrams in NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8dLcIhJyh8F",
        "colab_type": "text"
      },
      "source": [
        "4a. in-utterance ngram frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjRetUkdtfVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "26adebef-902e-48f7-8480-14a4848bf6b8"
      },
      "source": [
        "import nltk\n",
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "#counter for ngrams\n",
        "counts = collections.Counter()   # or nltk.FreqDist() ?\n",
        "for sent in tokenized_list:\n",
        "    counts.update(nltk.ngrams(sent, 2)) #change value to change n of ngrams\n",
        "\n",
        "fd=counts\n",
        "print(counts.most_common(100)) #change value to change the number of most common ngrams\n",
        "\n",
        "# NEW++NEW++NEW write complete counts output to a csv file \n",
        "df_counts = pd.DataFrame.from_dict(fd, orient='index')\n",
        "df_counts.columns = ['Frequency']\n",
        "df_counts.index.name = 'Term'\n",
        "df_counts.to_csv('absolute_bigram_frequency_in_young_m.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(('I', 'think'), 5328), (('I', \"don't\"), 5245), (('it', 'was'), 4624), (('you', 'know'), 4302), (('and', 'then'), 3809), (('in', 'the'), 3714), (('do', 'you'), 3393), (('I', 'was'), 3349), (('I', 'mean'), 2880), (('it', '?'), 2820), (('of', 'the'), 2720), (('pauseshort', 'I'), 2535), (('a', 'bit'), 2395), (('and', 'I'), 2352), (('yeah', 'I'), 2338), (('to', 'be'), 2296), (('yeah', 'pauseshort'), 2263), ((\"don't\", 'know'), 2246), (('you', 'can'), 2235), (('if', 'you'), 2194), (('on', 'the'), 2181), (('like', 'a'), 2141), (('sort', 'of'), 2077), (('kind', 'of'), 2058), (('have', 'to'), 2048), (('I', 'know'), 2039), ((\"it's\", 'not'), 1987), (('was', 'like'), 1956), (('yeah', 'yeah'), 1948), ((\"it's\", 'a'), 1740), (('but', 'I'), 1735), (('a', 'lot'), 1734), (('as', 'well'), 1690), (('to', 'do'), 1682), (('pauseshort', 'and'), 1680), (('is', 'it'), 1676), (('have', 'a'), 1638), ((\"it's\", 'like'), 1634), (('it', 'is'), 1539), (('going', 'to'), 1516), (('that', 'was'), 1502), (('he', 'was'), 1486), (('at', 'the'), 1468), (('what', '?'), 1465), (('to', 'the'), 1425), (('oh', 'yeah'), 1422), (('I', 'just'), 1380), (('no', 'no'), 1361), (('are', 'you'), 1342), (('or', 'something'), 1338), (('I', 'I'), 1317), (('like', 'that'), 1310), (('you', 'have'), 1302), (('need', 'to'), 1291), (('to', 'get'), 1289), (('this', 'is'), 1280), ((\"I'm\", 'not'), 1259), (('so', 'I'), 1254), (('is', 'that'), 1252), ((\"it's\", 'just'), 1248), (('was', 'a'), 1227), (('in', 'a'), 1208), (('got', 'a'), 1193), (('like', 'the'), 1185), (('do', 'it'), 1153), (('I', 'can'), 1151), (('did', 'you'), 1145), (('I', 'have'), 1141), (('all', 'the'), 1126), (('one', 'of'), 1118), (('anoninfo', '?'), 1114), (('no', 'I'), 1102), (('I', \"can't\"), 1098), (('and', 'you'), 1097), (('know', 'what'), 1096), (('to', 'go'), 1090), (('pauseshort', 'so'), 1083), (('yeah', 'but'), 1070), (('have', 'you'), 1063), (('the', 'same'), 1059), (('you', 'get'), 1055), (('pauseshort', \"it's\"), 1042), (('you', '?'), 1031), (('you', \"don't\"), 1029), (('just', 'like'), 1015), (('pauseshort', 'yeah'), 1014), (('pauseshort', 'but'), 1014), (('lot', 'of'), 1014), (('and', 'it'), 1010), (('for', 'a'), 1002), (('I', \"didn't\"), 997), (('that', '?'), 996), ((\"I've\", 'got'), 995), (('and', 'the'), 981), ((\"don't\", 'think'), 968), (('what', 'I'), 951), (('and', 'they'), 946), (('when', 'I'), 944), ((\"isn't\", 'it'), 940), (('like', 'I'), 938)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "modsu49FzGJj",
        "colab_type": "text"
      },
      "source": [
        "4b. full utterances as ngrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3g7DB-fzEYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d8bd8642-e3c3-4f3b-8e49-0254df3c297c"
      },
      "source": [
        "#in this section we extract distribution frequencies for whole utterances\n",
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "\n",
        "#unigram (=utterance)\n",
        "fd_u = nltk.FreqDist(one_line_list)\n",
        "print(fd.most_common(50))\n",
        "\n",
        "#bigram\n",
        "#bigram_fd = nltk.FreqDist(nltk.bigrams(one_line_list))\n",
        "#bigram_fd.most_common()\n",
        "\n",
        "#this solution looks at the most common utterances (if unigram), \n",
        "#and most common turns (if bigram) (?)\n",
        "\n",
        "# NEW++NEW++NEW write complete utterance frequency output to a csv file \n",
        "df_u = pd.DataFrame.from_dict(fd_u, orient='index')\n",
        "df_u.columns = ['Frequency']\n",
        "df_u.index.name = 'Term'\n",
        "df_u.to_csv('absolute_utterance_frequency_in_young_m.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('yeah', 52098), ('mm', 34816), ('vocallaugh', 15356), ('no', 7162), ('oh', 6259), ('unclearutt', 5838), ('yes', 5545), ('right', 3708), ('yeah yeah', 2841), ('oh right', 2455), ('okay', 2438), ('mm mm', 1558), ('ah', 1479), ('oh yeah', 1388), ('erm', 1381), ('really?', 1256), ('anoninfo', 1129), ('well', 1061), ('uhu', 971), ('oh okay', 871), ('yeah?', 862), ('so', 810), ('what?', 808), ('yeah vocallaugh', 773), ('mm?', 759), ('I know', 731), ('yeah pauseshort yeah', 722), ('wow', 629), ('er', 574), ('mm hm', 570), ('no no', 556), ('oh no', 530), ('mhm', 515), (\"I don't know\", 504), ('oh dear', 502), (\"that's right\", 496), ('you know', 488), ('oh really?', 454), ('vocalcough', 449), ('mm yeah', 442), ('vocalmisc', 436), ('but', 433), ('and', 407), ('is it?', 390), ('oh yes', 382), ('yep', 371), ('yeah yeah yeah', 367), ('mm pauseshort mm', 365), ('vocallaugh yeah', 363), ('why?', 340)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfEkFMcxYiGv",
        "colab_type": "text"
      },
      "source": [
        "# **Data visualization:**\n",
        "## paired bar plot to compare frequencies across two datasets\n",
        "- NEW: working seaborn plot\n",
        "- OLD: matlibplot with sorting problem (can probably be discarded)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6kez3cNlY7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###NEW NEW NEW### Paired bar plot with seaborn\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# read in csv files\n",
        "dfread1 = pd.read_csv('absolute_term_frequency_in_old_m.csv', index_col=None, header=0)\n",
        "dfread2 = pd.read_csv('absolute_term_frequency_in_old_f.csv', index_col=None, header=0)\n",
        "\n",
        "# add column with differentiator to both dataframes (male, female, old, young etc.)\n",
        "dfread1['Differentiator'] = 'male'\n",
        "dfread2['Differentiator'] = 'female'\n",
        "\n",
        "# sort dataframe by 'Frequency' column, and keep only top 100 entries\n",
        "sorted_top_dfread1 = dfread1.sort_values('Frequency',ascending=False).head(100)\n",
        "sorted_top_dfread2 = dfread2.sort_values('Frequency',ascending=False).head(100)\n",
        "\n",
        "# merge both dataframes into one\n",
        "raw_data = pd.concat([sorted_top_dfread1,sorted_top_dfread2])\n",
        "\n",
        "#prints the first 5 rows for checking\n",
        "print(raw_data.head())\n",
        "\n",
        "# draw paired bar plot: we want to investigate the frequency by term\n",
        "# plotting total frequency by term by differentiator\n",
        "plt.figure(figsize=(45,15))\n",
        "plt.xticks(rotation=45)\n",
        "ax = sns.barplot(x='Term', y='Frequency', data=raw_data, ci=False, hue = 'Differentiator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIBGqw0iYhwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### OLD PLOT WITH SORTING PROBLEM\n",
        "### UNIGRAMS BY AGERANGE ###\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# read in csv files\n",
        "dfread1 = pd.read_csv('absolute_term_frequency_in_old_m.csv', index_col=None, header=0)\n",
        "dfread2 = pd.read_csv('absolute_term_frequency_in_old_f.csv', index_col=None, header=0)\n",
        "\n",
        "\n",
        "# sort df by n of \"frequency\" column, top 50\n",
        "sorted_top_dfread1 = dfread1.sort_values('Frequency',ascending=False).head(100)\n",
        "sorted_top_dfread2 = dfread2.sort_values('Frequency',ascending=False).head(100)\n",
        "\n",
        "# visualize as (paired values) bar plot\n",
        "df1 = pd.DataFrame(sorted_top_dfread1, columns=['Term', 'Frequency'])\n",
        "df2 = pd.DataFrame(sorted_top_dfread2, columns=['Term', 'Frequency'])\n",
        "\n",
        "df1['Key'] = 'female speaker'\n",
        "df2['Key'] = 'male speaker'\n",
        "\n",
        "paired_df = pd.concat([df1,df2],keys=['male speaker','female speaker'])\n",
        "#print (paired_df.info())\n",
        "print(paired_df)\n",
        "barpairs = paired_df.groupby(['Term','Key'])\n",
        "\n",
        "PairedFreqPlot = barpairs.sum().unstack('Key').plot(kind='bar', color=['pink', 'blue'], figsize=(25,5))\n",
        "#PairedFreqPlot = PairedFreqPlot.sort_values(paired_df[('female_speaker','Frequency')], ascending = False) #sorting doesnt work :( "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPBaW2eENEm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "######   ######   ######   TEST ENV FOR VISUALIZATION   ######   ######   ######  \n",
        "################################################################################\n",
        "\n",
        "#visuals with seaborn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "#loading the data\n",
        "df1 = pd.read_csv('absolute_term_frequency_in_agerange_old.csv')\n",
        "#df2 = pd.read_csv('absolute_term_frequency_in_agerange_young.csv')\n",
        "\n",
        "#checking the structure\n",
        "#df1.head()\n",
        "#checking the shape\n",
        "#df1.shape\n",
        "#checking the columns\n",
        "df1.columns\n",
        "\n",
        "#ax = sns.barplot(x=\"Term\", y=\"Frequency\", data=df1)\n",
        "ax = sns.barplot(x='Term', y=\"Frequency\", hue=None, data=df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYKvxknuLh0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### BIGRAMS BY AGERANGE ###\n",
        "\n",
        "# read in csv files\n",
        "dfread1 = pd.read_csv('absolute_bigram_frequency_in_agerange_old.csv', index_col=None, header=0)\n",
        "dfread2 = pd.read_csv('absolute_bigram_frequency_in_agerange_young.csv', index_col=None, header=0)\n",
        "\n",
        "# sort df by n of \"frequency\" column, top 100\n",
        "sorted_top_dfread1 = dfread1.sort_values('Frequency',ascending=False).head(100)\n",
        "sorted_top_dfread2 = dfread2.sort_values('Frequency',ascending=False).head(100)\n",
        "\n",
        "# visualize as (paired values) bar plot\n",
        "df1 = pd.DataFrame(sorted_top_dfread1, columns=['Term', 'Frequency'])\n",
        "df2 = pd.DataFrame(sorted_top_dfread2, columns=['Term', 'Frequency'])\n",
        "\n",
        "df1['Key'] = 'female speaker'\n",
        "df2['Key'] = 'male speaker'\n",
        "\n",
        "paired_df = pd.concat([df1,df2],keys=['male speaker','female speaker'])\n",
        "\n",
        "barpairs = paired_df.groupby(['Term','Key'])\n",
        "\n",
        "PairedFreqPlot = barpairs.sum().unstack('Key').plot(kind='bar', color=['pink', 'blue'], figsize=(25,5))\n",
        "#PairedFreqPlot = PairedFreqPlot.sort_values('Frequency') #sorting doesnt work :( "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5AZ57_QLpTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TRIGRAMS BY AGERANGE ###\n",
        "\n",
        "# read in csv files\n",
        "dfread1 = pd.read_csv('absolute_trigram_frequency_in_agerange_old.csv', index_col=None, header=0)\n",
        "dfread2 = pd.read_csv('absolute_trigram_frequency_in_agerange_young.csv', index_col=None, header=0)\n",
        "\n",
        "\n",
        "# sort df by n of \"frequency\" column, top 50\n",
        "sorted_top_dfread1 = dfread1.sort_values('Frequency',ascending=False).head(100)\n",
        "sorted_top_dfread2 = dfread2.sort_values('Frequency',ascending=False).head(100)\n",
        "\n",
        "# visualize as (paired values) bar plot\n",
        "df1 = pd.DataFrame(sorted_top_dfread1, columns=['Term', 'Frequency'])\n",
        "df2 = pd.DataFrame(sorted_top_dfread2, columns=['Term', 'Frequency'])\n",
        "\n",
        "df1['Key'] = 'female speaker'\n",
        "df2['Key'] = 'male speaker'\n",
        "\n",
        "paired_df = pd.concat([df1,df2],keys=['male speaker','female speaker'])\n",
        "\n",
        "barpairs = paired_df.groupby(['Term','Key'])\n",
        "\n",
        "PairedFreqPlot = barpairs.sum().unstack('Key').plot(kind='bar', color=['pink', 'blue'], figsize=(25,5))\n",
        "#PairedFreqPlot = PairedFreqPlot.sort_values('Frequency') #sorting doesnt work :( "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW6aGKljL3nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### BONUS: WHOLE UTTERANCE BY AGERANGE ###\n",
        "\n",
        "# read in csv files\n",
        "dfread1 = pd.read_csv('absolute_utterance_frequency_in_agerange_old.csv', index_col=None, header=0)\n",
        "dfread2 = pd.read_csv('absolute_utterance_frequency_in_agerange_young.csv', index_col=None, header=0)\n",
        "\n",
        "\n",
        "# sort df by n of \"frequency\" column, top 50\n",
        "sorted_top_dfread1 = dfread1.sort_values('Frequency',ascending=False).head(100)\n",
        "sorted_top_dfread2 = dfread2.sort_values('Frequency',ascending=False).head(100)\n",
        "\n",
        "# visualize as (paired values) bar plot\n",
        "df1 = pd.DataFrame(sorted_top_dfread1, columns=['Term', 'Frequency'])\n",
        "df2 = pd.DataFrame(sorted_top_dfread2, columns=['Term', 'Frequency'])\n",
        "\n",
        "df1['Key'] = 'female speaker'\n",
        "df2['Key'] = 'male speaker'\n",
        "\n",
        "paired_df = pd.concat([df1,df2],keys=['male speaker','female speaker'])\n",
        "\n",
        "barpairs = paired_df.groupby(['Term','Key'])\n",
        "\n",
        "PairedFreqPlot = barpairs.sum().unstack('Key').plot(kind='bar', color=['pink', 'blue'], figsize=(25,5))\n",
        "#PairedFreqPlot = PairedFreqPlot.sort_values('Frequency') #sorting doesnt work :( "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HMWJBRI2xAx",
        "colab_type": "text"
      },
      "source": [
        "# **Data processing and cleaning (done)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlEJPFDVOpxQ",
        "colab_type": "text"
      },
      "source": [
        "Import dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCsbyVqgOocq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y_amkUWOQT2",
        "colab_type": "text"
      },
      "source": [
        "Remove and change content of csv. files with replace and regex \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwl4YTrlN9dY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "33337ff1-41af-4b1a-ddd5-54d2e9338e6e"
      },
      "source": [
        "path = r'/home/andreas/Desktop/SpokenBNC/spoken/testclean/'\n",
        "path_to_output = r'/home/andreas/PycharmProjects/spokenbncdataset/cleaning/cleaned/'\n",
        "\n",
        "# # Change filename here to load testframe small or big\n",
        "#df = pd.read_csv('testframe_big.csv', index_col=None, header=0)\n",
        "df = pd.read_csv('oneframe.csv', index_col=None, header=0)\n",
        "\n",
        "# # list with patterns that are deleted\n",
        "To_remove_lst = [\"</u>\", \"<unclear>\", \"</unclear>\"]\n",
        "To_change_short_pause = [\"<pause dur=\\\"short\\\" />\"]\n",
        "To_change_long_pause = [\"<pause dur=\\\"long\\\" />\"]\n",
        "To_change_unclearutt = [\"<unclear />\"]\n",
        "To_change_startsinging = [\"<shift new=\\\"singing\\\" />\"]\n",
        "To_change_endsinging = [\"<shift new=\\\"normal\\\" />\"]\n",
        "To_change_truncbegin = [\"<trunc>\"]\n",
        "To_change_truncend = [\"</trunc>\"]\n",
        "\n",
        "#find pattern from list above and remove it\n",
        "df['utterance'] = df['utterance'].str.replace('|'.join(To_remove_lst), '')\n",
        "df['utterance'] = df['utterance'].str.replace('|'.join(To_change_short_pause), 'pauseshort')\n",
        "df['utterance'] = df['utterance'].str.replace('|'.join(To_change_long_pause), 'pauselong')\n",
        "df['utterance'] = df['utterance'].str.replace('|'.join(To_change_unclearutt), 'unclearutt')\n",
        "df['utterance'] = df['utterance'].str.replace('|'.join(To_change_startsinging), 'startsinging')\n",
        "df['utterance'] = df['utterance'].str.replace('|'.join(To_change_endsinging), 'endsinging')\n",
        "df['utterance'] = df['utterance'].str.replace('|'.join(To_change_truncbegin), 'truncstart ')\n",
        "df['utterance'] = df['utterance'].str.replace('|'.join(To_change_truncend), ' truncend')\n",
        "\n",
        "# re.sub to remove eveything up to first occurance of \">\"\n",
        "df['utterance'] = [re.sub(\"^.*?>\", \"\", str(x)) for x in df['utterance']]\n",
        "\n",
        "# re.sub with custom function to subsitute \"event desc\" with event+description of event in one word\n",
        "def my_replace_for_event(match):\n",
        "    match = match.group()\n",
        "    return re.sub(\"\\W|desc\", \"\", str(match))\n",
        "\n",
        "df['utterance'] = [re.sub(\"<event desc=\\\"(\\w*(\\s?\\w*)*\\\")\\s\\/>\", my_replace_for_event, str(x)) for x in df['utterance']]\n",
        "\n",
        "# re.sub with custom function to subsitute \"vocal desc\" with vocal+description of vocal in one word\n",
        "def my_replace_for_vocal(match):\n",
        "    match = match.group()\n",
        "    return re.sub(\"\\W|desc\", \"\", str(match))\n",
        "\n",
        "df['utterance'] = [re.sub(\"<vocal desc=\\\"(\\w*(\\s?\\w*)*\\\")\\s\\/>\", my_replace_for_vocal, str(x)) for x in df['utterance']]\n",
        "\n",
        "# re.sub with custom function to delete the foreign language tags, but keep the foreign words and sentences\n",
        "def my_replace_for_foreignstart(match):\n",
        "    match = match.group()\n",
        "    return re.sub(\"\\<foreign\\slang=\\\"\\w\\w\\w\\\"\\>\", \"\", str(match))\n",
        "df['utterance'] = [re.sub(\"\\<foreign\\slang=\\\"\\w\\w\\w\\\"\\>\", my_replace_for_foreignstart, str(x)) for x in df['utterance']]\n",
        "\n",
        "def my_replace_for_foreignend(match):\n",
        "    match = match.group()\n",
        "    return re.sub(\"\\<\\/foreign\\>\", \"\", str(match))\n",
        "df['utterance'] = [re.sub(\"\\<\\/foreign\\>\", my_replace_for_foreignend, str(x)) for x in df['utterance']]\n",
        "\n",
        "def my_replace_for_foreignempty(match):\n",
        "    match = match.group()\n",
        "    return re.sub(\"\\<foreign\\slang=\\\"\\w\\w\\w\\\"\\s\\/\\>\", \"\", str(match))\n",
        "df['utterance'] = [re.sub(\"\\<foreign\\slang=\\\"\\w\\w\\w\\\"\\s\\/\\>\", my_replace_for_foreignempty, str(x)) for x in df['utterance']]\n",
        "\n",
        "# re.sub with custom function to clean the anonymized male name tags\n",
        "def my_replace_for_anon(match):\n",
        "    match = match.group()\n",
        "    return re.sub(\"<anon ((type|nameType)=\\\"\\w*\\\" ?)* \\/>\", \"anoninfo\", str(match))\n",
        "df['utterance'] = [re.sub(\"<anon ((type|nameType)=\\\"\\w*\\\" ?)* \\/>\", my_replace_for_anon, str(x)) for x in df['utterance']]\n",
        "\n",
        "#print(df)\n",
        "\n",
        "# check for pesky \">\" or \"<\" in dataframe:\n",
        "#print('people' in df)\n",
        "\n",
        "found = df[df['utterance'].str.contains('<')]\n",
        "print(found.count())\n",
        "\n",
        "# write csv\n",
        "df.to_csv('cleanframe.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UID          0\n",
            "speaker      0\n",
            "utterance    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC5sYavtvW_L",
        "colab_type": "text"
      },
      "source": [
        "## TEST ENV ## TEST ENV ## TEST ENV ## TEST ENV ## TEST ENV ## TEST ENV ## TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T06v0DQMvWWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHCCoVO3IK-X",
        "colab_type": "text"
      },
      "source": [
        "## SPLIT DATAFRAME USING SPEAKER IDS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Fw5g3GdbGL",
        "colab_type": "text"
      },
      "source": [
        "Part 1: read metadata file in and create lists (as pickle objects) with various SpeakerIDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9spg0Boh_K_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a457bf8e-302c-4263-a399-faca04f6d454"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "#input output paths\n",
        "path = r'/home/andreas/Desktop/SpokenBNC/spoken/metadata/'\n",
        "path_to_output = r'/home/andreas/PycharmProjects/spokenbncdataset/cleaning/cleaned/'\n",
        "\n",
        "#read dataframe and add column names\n",
        "df = pd.read_csv('bnc2014spoken-speakerdata.tsv', index_col=None, header=None, sep='\\t', names=['SpeakerID', 'exactage', 'age1994', 'agerange', 'gender', 'nat',  'birthplace', 'birthcountry', 'l1', 'lingorig', 'dialect_rep',\n",
        "                                            'hab_city', 'hab_country', 'hab_dur', 'dialect_l1', 'dialect_l2', 'dialect_l3', 'dialect_l4', 'edqual', 'occupation', 'socgrade', 'nssec',\n",
        "                                           'l2','fls','in_core'])\n",
        "\n",
        "# selecting rows by column name based on condition and save as list\n",
        "# gender m and f\n",
        "ID_list_gender_m = df[df['gender'] == 'M']['SpeakerID'].to_list()\n",
        "ID_list_gender_f = df[df['gender'] == 'F']['SpeakerID'].to_list()\n",
        "\n",
        "## age above and below 30\n",
        "agerange_young = ['0_10', '11_18', '19_29']\n",
        "agerange_old = ['30_39', '40_49', '50_59', '60_69', '70_79', '80_89', '90_99']\n",
        "\n",
        "ID_list_agerange_young = df.query('(agerange in @agerange_young)')['SpeakerID'].to_list()\n",
        "ID_list_agerange_old = df.query('(agerange in @agerange_old)')['SpeakerID'].to_list()\n",
        "\n",
        "## age and gender combined (can be edited to include other values eg SES)\n",
        "gender_female = ['F']\n",
        "gender_male = ['M']\n",
        "ID_list_old_f = df.query('(agerange in @agerange_old) & (gender in @gender_female)')['SpeakerID'].to_list()\n",
        "ID_list_old_m = df.query('(agerange in @agerange_old) & (gender in @gender_male)')['SpeakerID'].to_list()\n",
        "ID_list_young_f = df.query('(agerange in @agerange_young) & (gender in @gender_female)')['SpeakerID'].to_list()\n",
        "ID_list_young_m = df.query('(agerange in @agerange_young) & (gender in @gender_male)')['SpeakerID'].to_list()\n",
        "\n",
        "# ## print n and speaker ID of lists\n",
        "#print('List of Speaker IDs: ', ID_list_gender_f)\n",
        "print('n of female is:', len(ID_list_gender_f))\n",
        "print('n of male is:', len(ID_list_gender_m))\n",
        "print('n of old is:', len(ID_list_agerange_old))\n",
        "print('n of young is:', len(ID_list_agerange_young))\n",
        "print('n of young+female is:', len(ID_list_young_f))\n",
        "print('n of young+male is:', len(ID_list_young_m))\n",
        "print('n of old+female is:', len(ID_list_old_f))\n",
        "print('n of old+male is:', len(ID_list_old_m))\n",
        "\n",
        "## save any lst with pickle\n",
        "with open('outfile', 'wb') as fp:\n",
        "    pickle.dump(ID_list_old_f, fp)\n",
        "print('Selected IDs successfully saved as pickle outfile')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n of female is: 365\n",
            "n of male is: 305\n",
            "n of old is: 363\n",
            "n of young is: 299\n",
            "n of young+female is: 173\n",
            "n of young+male is: 126\n",
            "n of old+female is: 189\n",
            "n of old+male is: 174\n",
            "Selected IDs successfully saved as pickle outfile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd1YZPgndzIr",
        "colab_type": "text"
      },
      "source": [
        "Part 2: read lists with speakerIDs (using pickle) and split oneframe.csv accordingly into smaller frames, e.g. onle male, only female, only old, only young , female+young, male+old etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9aNstGodyDO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8652521a-d20a-4f89-9de8-fb34e1ef7f92"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "#input output paths\n",
        "path = r'/home/andreas/Desktop/SpokenBNC/spoken/testclean/'\n",
        "path_to_output = r'/home/andreas/PycharmProjects/spokenbncdataset/subcorpora/'\n",
        "\n",
        "#read complete csv\n",
        "df = pd.read_csv('cleanframe.csv', index_col=None, header=0)\n",
        "\n",
        "## load pickle outline from speaker_metadata file\n",
        "with open ('outfile', 'rb') as fp:\n",
        "    selected_speakerIDs = pickle.load(fp)\n",
        "\n",
        "##create empty df and fill with rows featuring selected speaker IDs, then save to disk\n",
        "df2 = pd.DataFrame()\n",
        "splitframe = df.query('(speaker in @selected_speakerIDs)').append(df2, ignore_index = True)\n",
        "splitframe.to_csv('age_old.csv', index=False)\n",
        "print(splitframe)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        UID speaker                                          utterance\n",
            "0         5   S0688                                       unclearutt\\n\n",
            "1         7   S0688        they don't have the the one doesn't exist\\n\n",
            "2        12   S0688                  yeah but it came to eighty-four\\n\n",
            "3        21   S0688                                             five\\n\n",
            "4        24   S0688                                       yeah you'd\\n\n",
            "...     ...     ...                                                ...\n",
            "403685  369   S0232  cos cos we lied about anoninfo's age vocallaugh\\n\n",
            "403686  371   S0232  yeah we erm I just said well cos they said obv...\n",
            "403687  373   S0232  but we well we'll pay ten pound we can either ...\n",
            "403688  375   S0232  hello take your shoes off did daddy put someth...\n",
            "403689  378   S0232  your trousers are soaked you're gonna have to ...\n",
            "\n",
            "[403690 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOEovoChhiwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "df4fed68-d44b-4589-ebcc-37f03baa6fda"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text.read(one_line_list.txt)\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "print(token_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-913d9512f378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_line_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#  \"nlp\" Object is used to create documents with linguistic annotations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
          ]
        }
      ]
    }
  ]
}